# Guide d'utilisation d'Ollama avec Bo√Æte √† Outils

## ‚ö†Ô∏è Important : Lancer l'application correctement

Pour √©viter les erreurs CORS, **ne double-clique PAS sur index.html** !

√Ä la place, lance l'application avec :
```bash
./start.sh
```

Ou manuellement :
```bash
python3 -m http.server 8080
# Puis ouvre http://localhost:8080 dans ton navigateur
```

## Pourquoi Ollama ?

Ollama vous permet d'utiliser des LLMs (Large Language Models) **localement sur votre machine**, sans envoyer vos donn√©es √† des services externes et **sans co√ªts d'API**. C'est une excellente alternative √† Gemini pour :

- üí∞ **√âconomiser des tokens** - Aucun co√ªt d'API
- üîí **Confidentialit√©** - Vos donn√©es restent sur votre machine
- ‚ö° **Pas de quota** - Utilisez autant que vous voulez
- üåê **Hors ligne** - Fonctionne sans connexion internet

## Installation d'Ollama

### 1. Installer Ollama

**Sur macOS** :
```bash
brew install ollama
```

**Sur Linux** :
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Sur Windows** :
T√©l√©chargez l'installeur depuis https://ollama.com/download

### 2. T√©l√©charger un mod√®le

Ollama propose plusieurs mod√®les. Voici les recommandations :

**Mod√®les recommand√©s** :
- `llama3.2` (7B) - Bon √©quilibre performance/taille
- `llama3.2:1b` - Tr√®s rapide, moins pr√©cis
- `mistral` (7B) - Excellent pour le fran√ßais
- `qwen2.5:7b` - Tr√®s bon mod√®le g√©n√©ral
- `gemma2:9b` - Bon pour les t√¢ches complexes

Pour t√©l√©charger un mod√®le :
```bash
ollama pull llama3.2
```

### 3. Lancer Ollama

Ollama doit √™tre en cours d'ex√©cution pour que l'application puisse l'utiliser :

```bash
ollama serve
```

Ou lancez simplement un mod√®le (qui d√©marre automatiquement le serveur) :
```bash
ollama run llama3.2
```

## Configuration dans Bo√Æte √† Outils

1. Ouvrez l'application dans votre navigateur
2. Allez dans **Analyse IA**
3. S√©lectionnez **ü§ñ Ollama (LLM local)** dans le menu d√©roulant
4. Cliquez sur **‚öôÔ∏è Config Ollama**
5. V√©rifiez les param√®tres :
   - **Endpoint** : `http://localhost:11434` (par d√©faut)
   - **Mod√®le** : Le nom du mod√®le que vous avez t√©l√©charg√© (ex: `llama3.2`)

## Utilisation

Une fois configur√©, utilisez simplement l'interface **Analyse IA** comme d'habitude. Vos requ√™tes seront trait√©es localement par Ollama !

## D√©pannage

### Erreur "Impossible de contacter Ollama"
- Assurez-vous qu'Ollama est lanc√© : `ollama serve`
- V√©rifiez que l'endpoint est correct : `http://localhost:11434`

### Le mod√®le n'est pas trouv√©
- T√©l√©chargez-le : `ollama pull <nom-du-mod√®le>`
- V√©rifiez le nom exact du mod√®le avec : `ollama list`

### R√©ponses lentes
- Utilisez un mod√®le plus petit (ex: `llama3.2:1b` au lieu de `llama3.2`)
- V√©rifiez que votre machine a assez de RAM disponible

## Commandes utiles

```bash
# Lister les mod√®les install√©s
ollama list

# Supprimer un mod√®le
ollama rm <nom-du-mod√®le>

# Tester un mod√®le en ligne de commande
ollama run llama3.2 "Bonjour, comment vas-tu ?"

# Voir les logs d'Ollama
journalctl -u ollama  # Linux
# ou chercher dans Console.app sur macOS
```

## Recommandations

- Pour les **analyses rapides** : `llama3.2:1b` ou `mistral:7b-instruct`
- Pour la **meilleure qualit√©** : `qwen2.5:14b` ou `llama3.1:70b` (n√©cessite beaucoup de RAM)
- Pour le **fran√ßais** : `mistral` ou `vigogne`

## Ressources

- Site officiel : https://ollama.com
- Liste des mod√®les : https://ollama.com/library
- Documentation : https://github.com/ollama/ollama/blob/main/docs/README.md

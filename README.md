# ğŸ§° BoÃ®te Ã  Outils de Communication

Application web pour analyser et amÃ©liorer tes communications interpersonnelles difficiles. Transforme les crises relationnelles en apprentissages actionnables avec des scripts de rÃ©ponse prÃªts Ã  l'emploi.

## ğŸš€ DÃ©marrage rapide

### 1. Lancer l'application

**MÃ©thode recommandÃ©e** (Ã©vite les erreurs CORS) :
```bash
./start.sh
```

Ou manuellement :
```bash
python3 -m http.server 8080
# Puis ouvre http://localhost:8080 dans ton navigateur
```

âš ï¸ **Ne double-clique PAS sur index.html** directement, Ã§a ne fonctionnera pas avec Ollama Ã  cause des restrictions CORS.

### 2. Choisir ton provider IA

L'application supporte 3 modes d'analyse :

- **ğŸ” Analyse locale (gratuit)** - Heuristique simple, pas d'IA externe
- **ğŸ¤– Ollama (LLM local)** - IA puissante locale, gratuit, privÃ© â­ **RecommandÃ©**
- **âœ¨ Gemini API** - IA cloud performante, nÃ©cessite une clÃ© API (payant)

## ğŸ¤– Configuration Ollama (RecommandÃ©)

Ollama te permet d'utiliser des LLMs localement **sans frais et en privÃ©**.

### Installation

1. **Installer Ollama** :
   ```bash
   brew install ollama  # macOS
   # ou tÃ©lÃ©charge depuis https://ollama.com
   ```

2. **TÃ©lÃ©charger un modÃ¨le** :
   ```bash
   ollama pull llama3.2  # RecommandÃ© : bon Ã©quilibre
   # ou
   ollama pull mistral   # Excellent pour le franÃ§ais
   ```

3. **Lancer Ollama** :
   ```bash
   ollama serve
   # Ou simplement : ollama run llama3.2
   ```

4. **Dans l'application** :
   - SÃ©lectionne "ğŸ¤– Ollama (LLM local)"
   - Configure si besoin (âš™ï¸ Config Ollama)
   - C'est parti !

ğŸ“– Guide complet : [OLLAMA_GUIDE.md](OLLAMA_GUIDE.md)

## âœ¨ FonctionnalitÃ©s

### ğŸ“ Analyse Manuelle
Un assistant pas-Ã -pas en 4 Ã©tapes pour dÃ©cortiquer une situation difficile :
1. **Constat** - Capture le contexte brut
2. **Ego Radar** - Identifie l'ego dominant activÃ©
3. **MVP de rÃ©ponse** - Dessine ta rÃ©ponse idÃ©ale
4. **Action & Insight** - Plan d'action et leÃ§on clÃ©

### ğŸ¤– Analyse IA
Parse un message ou une situation avec l'IA de ton choix et obtiens :
- Une Ã©valuation du niveau de tension
- Des insights actionnables
- 2-3 scripts de rÃ©ponse prÃªts Ã  l'emploi

### ğŸ“” Journal
Toutes tes analyses sauvegardÃ©es localement :
- Filtre par type d'ego
- Export/Import JSON
- Historique complet

### ğŸ  Dashboard
Vue d'ensemble de ton Ã©volution :
- Nombre d'analyses
- Ego dominant identifiÃ©
- Streak sans "DÃ©fensive"
- Derniers insights

### ğŸ§­ Guide (Playbook)
- Glossaire des 5 types d'ego
- Framework de rÃ©ponse MVP
- Persona IA pour tes propres conversations

## ğŸ”’ ConfidentialitÃ©

- **DonnÃ©es 100% locales** : Tout est stockÃ© dans ton navigateur (localStorage)
- **Avec Ollama** : Aucune donnÃ©e ne sort de ta machine
- **Avec Gemini** : ClÃ© API chiffrÃ©e localement (AES-GCM)

## ğŸ› ï¸ Stack technique

- **Frontend** : HTML, CSS (Tailwind CDN), JavaScript vanilla
- **Storage** : localStorage (navigateur)
- **IA** :
  - Ollama (API locale)
  - Google Gemini API (optionnel)
  - Heuristique locale (fallback)

## ğŸ“‚ Structure

```
.
â”œâ”€â”€ index.html              # Page principale
â”œâ”€â”€ start.sh               # Script de lancement (recommandÃ©)
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ css/styles.css     # Styles personnalisÃ©s
â”‚   â””â”€â”€ js/app.js          # Application JavaScript
â”œâ”€â”€ README.md              # Ce fichier
â””â”€â”€ OLLAMA_GUIDE.md        # Guide Ollama dÃ©taillÃ©
```

## ğŸ› DÃ©pannage

### Erreur CORS avec Ollama
â¡ï¸ **Solution** : Lance l'app via `./start.sh` au lieu d'ouvrir `index.html` directement.

### Ollama ne rÃ©pond pas
```bash
# VÃ©rifie qu'Ollama tourne
curl http://localhost:11434/api/tags

# Si pas de rÃ©ponse, lance-le :
ollama serve
```

### Les modÃ¨les Ollama sont lents
â¡ï¸ Utilise un modÃ¨le plus petit :
```bash
ollama pull llama3.2:1b  # Version 1 milliard de paramÃ¨tres
```

## ğŸ“œ Licence

Usage personnel libre. Pour usage commercial, contacte l'auteur.

## ğŸ™ CrÃ©dits

InspirÃ© des approches de communication bienveillante et de rÃ©solution de conflits interpersonnels.
